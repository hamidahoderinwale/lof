[
  {
    "objectID": "sci_indexing.html",
    "href": "sci_indexing.html",
    "title": "New Gen Research Indexing",
    "section": "",
    "text": "Elevator pitch: Our research corpora is the collection of human insight. Certain aspects were thoughtfully constructed (i.e. citations and indexing algorithms), others were products of circumstance, and some are relics of the past. What does the ideal scientific literature, corpora, and search engine interface look like (in the generative age)? With the main goal being that researchers should be able to find the work that is the most valuable to them. An auxiliary goal being that automated reasoning systems (ML systems) can best take advantage of the literature to speed up the discovery process. In other words, what would a neo-Google Scholar look like?\n\n\nThe (key) audience in mind is the Google Scholar team; a secondary audience would be the Elicit AI team."
  },
  {
    "objectID": "sci_indexing.html#desiderataideas",
    "href": "sci_indexing.html#desiderataideas",
    "title": "New Gen Research Indexing",
    "section": "Desiderata/Ideas",
    "text": "Desiderata/Ideas\n\nTo be able to see how knowledge builds on each other by seeing how different concepts and artifacts build on each other and integrate into one another.\nTransparency on decisions related to an artifact (removals, etc.)\nLinking preliminary workshop papers to main journal and conference proceedings. The author being able to group different research projects.\nSeeing when components of a paper are re-used/cited, such as graphs and charts (This would require persistent identifiers for these graphs.) The tools that people use for making graphs and what is currently possible.\nHaving datasets linked on Google Scholar as well (dynamic (API-retrieved data) compared to static (CSV, instantiated datasets))\nCiting software and libraries (*release versions as metadata)\nCentralization (linking to OpenReview and the comments that a document would receive)\nResearch corpora demography: We’re now able to see the derivative cited works from a paper but how would this work for quotations (key points of an article that are recited as proxies for different concepts and their popularity)\n\nThis would allow for detailed similarity checks and being able to assess how original a work is compared to the broader literature\n\nHaving hashes for files on the arXiv as digital fingerprints\nEncouraging meta-analyses and visualizations of the google scholar dataset (see the ISBN visualization bounty)\nIn 2014, there were 160 million documents indexed [re: Wired article]. *Get the current number and compare across repositories (i.e. Elsevier, ArXiv, etc.)"
  },
  {
    "objectID": "sci_indexing.html#gscholar-history-background",
    "href": "sci_indexing.html#gscholar-history-background",
    "title": "New Gen Research Indexing",
    "section": "GScholar History & Background",
    "text": "GScholar History & Background\n\nDeveloped by Alex Verstak and Anurag Acharya (both still at Google)\nGoing through their most recent work and GScholar history:\n\nVerstak and Acharya filed a patent in 2017, approved in 2019 (?), (on behalf of Google) on a system for identifying the primary source of a document when there are mulitple versions of a document. As it is, internet search engines don’t have supervisory control which means that a multi-authored paper should be submitted by one of the editors to ensure that there is only one version and the history can be tracked as such. Their system involves an authority conferral for each version of a document obtained from different sources (i.e. different publishers, pre-print servers, repositories, etc.). The authoritative source is given based on the priority order of the source (with an algorithm consisting of parameters like page rank, citation, keywords, publisher exclusivity rights, and so on) and length qualification. The scores will indicate a ranking of priority in addition to meeting a length qualification. *Re SJ: It would be interesting to reconfigure the parameter weights and have this be visible to the user or allow them to reorder search results based on this.\nSeeking approval from publishers to allow Google to crawl their repositories and thereby allowing them to circumvent paywalls. While JSTOR (the largest archive of research then) provided scans of articles it didn’t “read the articles.” Initially, they could acquire the first page of the article and get author information and the abstract (I’m unsure if this is still the case today) *In what ways do current machine-readings come short?\nAlerts were an area of focus about a decade ago. What would be the meaningful things to be alerted on? See the derivative projects that came from a paper? *As the user, being able to refine the alerts that you receive. ## Auxiliary\n\nSome issues like if you web scrapers that surpass paywalls then the scientists don’t get paid (but that never really mattered: could run an experiment and see if that’s possible – jailbreaking)"
  },
  {
    "objectID": "sci_indexing.html#remarks-and-next-steps",
    "href": "sci_indexing.html#remarks-and-next-steps",
    "title": "New Gen Research Indexing",
    "section": "Remarks and Next Steps",
    "text": "Remarks and Next Steps\n\nWikidata relies on the Blazegraph which isn’t being mantained"
  },
  {
    "objectID": "draftdviz.html",
    "href": "draftdviz.html",
    "title": "Procedural Knowledge Libraries for Data Visualization Engineers",
    "section": "",
    "text": "Initially, this centered around procedural knowledge as a whole entity and now it’s information about the process of building new knowledge itself. Explicating these processes makes it possible to audit [human/machine] reasoners. You can imagine having repos of procedural knowledge for those analyzing (building models of) the same dataset. This framework would then make it possible to assess the mechanistic differences between their approaches, clearly defining the branches of thought that took place.\nThe important distinction to make here is that libraries are systems; they are dynamic, as the contents within them are living. To date, traditional “libraries” have assumed otherwise. Furthermore, the objects inside them are defined as an artifact of knowledge. That is, artifacts that are the product of thought and developed with a goal in mind. The function of the system is then to be a home to these artifacts and preserve them for prosperity’s sake, but more importantly for retrieval’s sake. As such, while the integrity of the work should be preserved, there are ways to build the system and store the objects such that this can be done.\nProvenance refers to the lineage of an object. Provenance, synonymous with metadata, has been developed with the static knowledge artifact in mind, so extending upon it for the sake of the dynamic artifact is key for a “new age of libraries.”\nNow, we are faced with the question of what it means to develop new knowledge. That is, what is something tractable than we can anchor to, to help build better libraries: libraries that work for knowledge that evolves over time. I use the instance of model-building. While there’s no dedicated library to models, they exist within research papers, repositories, and notebooks. Here, I outline the ways that we can capture how they are developed (e.g. improved version control, collaboratively, integrating “layered context architecture”), what they mean (i.e. to help people find exactly what they need by explicating what models and their requests mean), what it looks like to retrieve them (the ideal interface to help people search in accordance with their preferences), etc. And then I consider how they may fit into a larger system: how they may be cataloged, organized (e.g., the metadata that can be assigned to them, scalable metadata attribution, etc.), and stored (e.g. internet search engine that works for “findings.”).\nHere, we focus on the model-builder who is building graphical models or visualizations. Visualization is often the last step in conveying insight and analysis of data that has been obtained through the research process. Therefore, within this test-bed, we lay out a framework and suggest the limitations and trade-offs in real-world application (i.e. Log-based CRDTs being computationally expensive than traditional CRDTS), concluding with discussion on promising speculative and existing areas of work to address them."
  },
  {
    "objectID": "draftdviz.html#annotated-bib",
    "href": "draftdviz.html#annotated-bib",
    "title": "Procedural Knowledge Libraries for Data Visualization Engineers",
    "section": "9.1 Annotated Bib",
    "text": "9.1 Annotated Bib\n\nLearning to See by Looking at Noise\nA Framework for Considering Comprehensibility in Modelling\nIn this All of Us failed post (From UMAP blog posts). A grife of the author is that there were no descriptions: in the Rye paper there was little justification provided for the decisions that were made such as picking 16 principal components or What the difference would be between 20 principal components and 16. And there is no “general analysis describing the robustness of results [of the] parameter.” The author questions why the entirety of the “All of Us” consortium chose to use UMAP.\nhttps://arxiv.org/pdf/1802.03426v3\n\nThe paper mentions that “fuzzy topological representation” is a way to “merge the incompatible local views of the data.”\n\nhttps://arxiv.org/pdf/2111.15506 (Towards a comprehensive visualization of structure in data\n\nData transformations as described in the paper: 1) Take non-linear manifold in lower-dim where a visualization would be largely uninformative 2) then you take linear projections of the high-dim data and make it human-readable (2 or 3-d)\nThe problem is with non-linear methods which are computationally complex and less deterministic (?). Examples of such methods include t-SNE and UMAP.\nThey address this with (standardized?) parametric configs? They claim this would be generalizable to other non-linear methods.\nThere’s commonly trade-offs with capturing global structure compared to local structure. They propose a “retrieval information approach” where’s the neighbour retriever visualizer (NeRV) that looks at the cost of precision relative to recall. They do this in terms of retrieving/missing neighbors in the high-dim representation and the low-dim representation.\nYou’re also trading off speed compared to accuracy.\n\nI assume the premise is that chunking, discreteness, and cleaner parameters translate into better queries compared to more continuous data.\n\n\nhttps://www.pnas.org/doi/epdf/10.1073/pnas.95.25.14863)\nhttps://alarmingdevelopment.org/?p=1570\nhttps://www.tableau.com/sites/default/files/2023-01/2008-GraphicalHistories-InfoVis.pdf Graphical Histories for Visualization: Supporting Analysis, Communication, and Evaluation (2008)\nhttps://link.springer.com/article/10.1186/1471-2288-10-14 (Understanding human functioning using graphical models)"
  },
  {
    "objectID": "pkl_outline.html",
    "href": "pkl_outline.html",
    "title": "Introduction",
    "section": "",
    "text": "This research will apply the concept of the procedural knowledge library to the process of knowledge production itself.\nA library is a curated, organized collection of ideas, concepts, and facts, transformed into structured representations that users can access, interpret, and understand. It bridges raw information and human knowledge, providing tools to make meaning accessible. Libraries take many forms, as information can be encoded in text, sound, images, data, and more.\nWhile traditional libraries rely on static representations (e.g., printed books, fixed records), information is inherently dynamic. Today, we can also represent information dynamically—through interactive media, real-time data, and evolving digital archives. Libraries must adapt to these forms. Here, I focus on the digital medium and the dynamic artifacts it produces, such as the programs that live in repositories.\n\n\nI envision that the core tenet of modern knowledge work will be model-building—finding the right representations of our goals, specifying them, and iteratively refining them over time. Furthermore, with AI, traditionally imperative workflows will shift toward a more declarative paradigm: users will define their goals rather than specifying the steps required to achieve them.\nIn short, we have a significant opportunity to improve insight generation by using procedural knowledge capture to document how models are built and how they evolve over time."
  },
  {
    "objectID": "pkl_outline.html#preserving-the-model-development-process",
    "href": "pkl_outline.html#preserving-the-model-development-process",
    "title": "Introduction",
    "section": "",
    "text": "I envision that the core tenet of modern knowledge work will be model-building—finding the right representations of our goals, specifying them, and iteratively refining them over time. Furthermore, with AI, traditionally imperative workflows will shift toward a more declarative paradigm: users will define their goals rather than specifying the steps required to achieve them.\nIn short, we have a significant opportunity to improve insight generation by using procedural knowledge capture to document how models are built and how they evolve over time."
  },
  {
    "objectID": "pkl_outline.html#conceptual-framework",
    "href": "pkl_outline.html#conceptual-framework",
    "title": "Introduction",
    "section": "Conceptual framework",
    "text": "Conceptual framework\n\nThe paper will include the following:"
  },
  {
    "objectID": "pkl_outline.html#version-control-for-structured-graphical-data",
    "href": "pkl_outline.html#version-control-for-structured-graphical-data",
    "title": "Introduction",
    "section": "Version Control for Structured Graphical Data",
    "text": "Version Control for Structured Graphical Data\n\nVersion Control Repositories as Libraries\nWhile not traditionally viewed as such, version control systems like Git and their logs are libraries because they organize, preserve, and provide access to structured knowledge—documenting code evolution, decisions, and project histories. These dynamic artifacts act as repositories, lets users to interpret, understand, and build upon the work of others.\nVisual representations reveal patterns in complex data that text alone cannot. Exploratory data analysis serves as a systematic method for sense-making through these visualizations. This project proposes insight provenance—capturing not just final visualizations but the journey of analysis that produced them. At large, I see this is a project of capturing the reasoning process, in this case, using visualizations to make sense of complex datasets.\n\n\nImplementation\nThis project extends Chit, an experimental version-control system for structured data, to declarative graphical languages. Presently, developers using data visualization tools—such as Observable, Hex, and Jupyter notebooks—lack adequate version control systems suited for visual, exploratory, and iterative workflows. These workflows often involve open-ended methods to achieve an unstructured goal, making traditional file-based version control insufficient.\nI will apply this system to graphical representations, such as Vega-Lite–a high-level visualization grammar. As a library, it will incorporate version control to capture the process of creating and refining visualizations. Additionally, it will be extended to support core library functions, such as retrieving past history and enabling search across historical versions.\nGiven Chit’s experimental and early-stage nature, I plan to build my own understanding of its approach, extract its core principles, and develop a bespoke system tailored to procedural knowledge libraries. Where elements are retained from Chit, I will provide technical exposition to explain their design and rationale. 2\nWhile still in its early stages, Chit lays a strong foundation for the broader project and aligns with my vision for the library of the future: a system that works with dynamic artifacts—such as code-generated visualizations—whose context must be organized and preserved. Chit was originally developed to help build auditable, interpretable AI systems while being extensible to less-structured personal knowledge and logic languages (e.g., SQL).\n\n\nInforming the theory\nThis project serves as a test-bed to explore key questions that can inform the former:\n\nWhat trade-offs exist in designing an efficient yet expressive history-tracking system?\nHow can Chit handle visual representation changes, such as diffing and merging Vega-Lite specifications?\nHow could this extend to knowledge structures like RDF and OWL?\n\nThe final work will fully explain why this specific interpretation of a \"procedural knowledge library\" was worth exploring. I will outline the attributes of this particular instance of a procedural knowledge library informed by the conceptual framework above. This will test the universal properties of a procedural knowledge library, including: the types of procedures it supports, their medium of representation, the features of the goals it targets, and the decision to extend Chit rather than starting from scratch."
  },
  {
    "objectID": "pkl_outline.html#diffing-ml-model-evolution",
    "href": "pkl_outline.html#diffing-ml-model-evolution",
    "title": "Introduction",
    "section": "Diffing ML model evolution",
    "text": "Diffing ML model evolution\nModel diffing is a new ML interpretability technique in which LLMs are compared by analyzing their internal representations. We could potentially extend this by first having models generate graphical representations of their own internal structures (e.g. with TensorGraph). Then we apply model diffing and structured versioning techniques to the graphical representations they generate.\nThis approach could improve fine-tuning, enhance interpretability, and help researchers track model evolution over time. This builds on Gurnee and Tegmark’s (2024) work finding that models learn linear representations of space and time.\nWhile they manually visualized these representations, we might ask models to programmatically generate these visualizations themselves, perhaps expressing their reasoning in declarative graphical languages such as DOT (used by Graphviz), Mermaid, TikZ, D3.js, or ONNX. For example, a model could output its attention patterns as a Mermaid diagram, or express its learned spatial representations as TikZ code that researchers could then visualize, analyze, and compare across model versions."
  },
  {
    "objectID": "pkl_outline.html#model-interoperability",
    "href": "pkl_outline.html#model-interoperability",
    "title": "Introduction",
    "section": "Model Interoperability",
    "text": "Model Interoperability\n\nHow can denotational semantics—a method of mathematically formalizing the meaning of programming operations—serve as a common framework for managing models across different programming languages?\nHow can we combine declarative programming paradigms with denotational techniques to better capture developer intentions?\nWhat formal methods could be developed to ‘diff’ the semantic meaning and goals of code as it evolves over time, going beyond just analyzing syntactic changes?\nAdditionally, how can we enable reasoning about systems that mix formal, semi-formal, and informal representations?"
  },
  {
    "objectID": "pkl_outline.html#footnotes",
    "href": "pkl_outline.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nInspired by survey papers from ACM Transactions on Knowledge Discovery from Data: 1, 2.↩︎\nFor an example of a theory-driven implementation paper, see Designing Datalog-Based Embedded Languages at arXiv:2305.14773.↩︎"
  }
]